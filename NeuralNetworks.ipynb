{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORSuvG8my6MGuJ/BEN/uWB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucascerfig/PyTorch-experiments/blob/master/NeuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SImncMWEH8gI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz4SxBW4IqVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  # A classe Net sendo criada está herdando as propriedade de nn.Module\n",
        "  def __init__(self): # Construtor\n",
        "\n",
        "    super(Net, self).__init__() \n",
        "\n",
        "    # Conv2d(Canais de Entrada, Canais de Saída (Kernels), Tamanho dos Kernels)\n",
        "    self.conv1 = nn.Conv2d(1, 6, 3) # Camada 1 (Entrada?)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 3) # Camada 2 \n",
        "\n",
        "    # Operação linear (y = Wx + b)\n",
        "    # Linear(Tamanho da Entrada, Tamanho da Saída)\n",
        "    self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6x6 -> Dimensão da imagem\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  # Definição do Forward Pass\n",
        "  def forward(self, x): # A entrada x é um tensor\n",
        "    # Max Pooling numa janela 2x2\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), 2) # Se a entrada é quadrada, apenas um valor pode ser especificado\n",
        "    x = x.view(-1, self.num_flat_features(x)) # Extrai-se a matriz de features\n",
        "    x = F.relu(self.fc1(x)) # Operação Linear -> Operação Não-Linear (ReLU)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x) # A saída é um vetor com 10 valores\n",
        "    return x\n",
        "\n",
        "  def num_flat_features(self, x):\n",
        "    size = x.size()[1:] # Retorna as dimensões exceto o valor de batch\n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "      num_features *= s\n",
        "    return num_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYJU5D7pQZ8r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "7000911f-6b53-42f0-d210-8939ecec47a6"
      },
      "source": [
        "net = Net()\n",
        "print(net)\n",
        "\n",
        "# Basta definir a função forward, que a função backward já é automaticamente definida a partir do autograd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr9yBNrLQdd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c04a06b2-a50d-454e-815c-d2e5465e199e"
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size()) # Matriz de pesos dos kernels da primeira convolução"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "torch.Size([6, 1, 3, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OoHUfPbRAMu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "88410c56-cd6a-4217-c80e-006203bed3ee"
      },
      "source": [
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0435, -0.0710,  0.0878, -0.0065, -0.0012, -0.0818, -0.1535, -0.0436,\n",
            "         -0.1194,  0.0507]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qNwbGMESF22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.zero_grad() # Zerando o gradiente da rede\n",
        "out.backward(torch.randn(1, 10)) # Efetuando o backpropagation com gradientes aleatórios "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGUKnoulSGny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.Tensor - Vetor multidimentsionar que suporta operações do autograd como backward() e guarda o gradiente em relacao ao tensor.\n",
        "\n",
        "# nn.Module - Modúlo de redes neurais. Método conveniente de encapsular parâmetros, \n",
        "#             com métodos auxiliares para utilização da GPU, exportação, carregamento, etc.\n",
        "\n",
        "# nn.Parameter - Um tipo de Tensor, é automaticamente registrado como um parametro ao ser designado como um atributo de um Module.\n",
        "\n",
        "# autograd.Function - Define forward e backward nas operaçõos do autograd. Cada operação de um tensor cria pelo menos um nó de Function\n",
        "#                     que se conecta a funções que criaram o tensor e codifica seu histórico"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0gIyUu6UQGd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7f506b2-40b1-450a-f983-c7b3e1ffab03"
      },
      "source": [
        "output = net(input)\n",
        "target = torch.randn(10) # Valores alvo aleatórios\n",
        "target = target.view(1, -1) # Transformar na mesma dimensão da saída\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.0012, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_YHB-ApUum5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "da42ff9c-ce3d-4e33-ef7e-3bc22e201188"
      },
      "source": [
        "# Seguindo o calculo de loss\n",
        "print(loss.grad_fn)\n",
        "print(loss.grad_fn.next_functions[0][0])\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MseLossBackward object at 0x7f31937997b8>\n",
            "<AddmmBackward object at 0x7f3193799828>\n",
            "<AccumulateGrad object at 0x7f31937997b8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUmNmaZwU23h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c39412b8-553b-46a2-f38e-971acd40f43b"
      },
      "source": [
        "net.zero_grad() # Zera os gradientes de todos os parâmetros para não acumula-los aos novos gradientes\n",
        "\n",
        "print('conv1.bias.grad before backprop')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backprop')\n",
        "print(net.conv1.bias.grad)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.bias.grad before backprop\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "conv1.bias.grad after backprop\n",
            "tensor([ 0.0237, -0.0149,  0.0257, -0.0082,  0.0101, -0.0074])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loCQmxvXWD-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Montando um simples SGD\n",
        "learning_rate = 0.01\n",
        "for f in net.parameters(): \n",
        "  f.data.sub_(f.grad.data * learning_rate) # Aplicando a cada parametro da rede"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWYekkeFWtAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Criando um optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQQHNKezWuxR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e56c14f5-1450-4b6e-fe81-a93457f6d7fe"
      },
      "source": [
        "# Loop de Treinamento englobando as funções definidas\n",
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  output = net(input)\n",
        "  loss = criterion(output, target)\n",
        "  print(\"Epoch \", i + 1, \" loss: \", loss)\n",
        "  loss.backward()\n",
        "  optimizer.step() # Atualiza os pesos"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1  loss:  tensor(0.8795, grad_fn=<MseLossBackward>)\n",
            "Epoch  2  loss:  tensor(0.8712, grad_fn=<MseLossBackward>)\n",
            "Epoch  3  loss:  tensor(0.8629, grad_fn=<MseLossBackward>)\n",
            "Epoch  4  loss:  tensor(0.8549, grad_fn=<MseLossBackward>)\n",
            "Epoch  5  loss:  tensor(0.8469, grad_fn=<MseLossBackward>)\n",
            "Epoch  6  loss:  tensor(0.8390, grad_fn=<MseLossBackward>)\n",
            "Epoch  7  loss:  tensor(0.8308, grad_fn=<MseLossBackward>)\n",
            "Epoch  8  loss:  tensor(0.8228, grad_fn=<MseLossBackward>)\n",
            "Epoch  9  loss:  tensor(0.8143, grad_fn=<MseLossBackward>)\n",
            "Epoch  10  loss:  tensor(0.8057, grad_fn=<MseLossBackward>)\n",
            "Epoch  11  loss:  tensor(0.7972, grad_fn=<MseLossBackward>)\n",
            "Epoch  12  loss:  tensor(0.7884, grad_fn=<MseLossBackward>)\n",
            "Epoch  13  loss:  tensor(0.7793, grad_fn=<MseLossBackward>)\n",
            "Epoch  14  loss:  tensor(0.7699, grad_fn=<MseLossBackward>)\n",
            "Epoch  15  loss:  tensor(0.7605, grad_fn=<MseLossBackward>)\n",
            "Epoch  16  loss:  tensor(0.7507, grad_fn=<MseLossBackward>)\n",
            "Epoch  17  loss:  tensor(0.7407, grad_fn=<MseLossBackward>)\n",
            "Epoch  18  loss:  tensor(0.7303, grad_fn=<MseLossBackward>)\n",
            "Epoch  19  loss:  tensor(0.7199, grad_fn=<MseLossBackward>)\n",
            "Epoch  20  loss:  tensor(0.7087, grad_fn=<MseLossBackward>)\n",
            "Epoch  21  loss:  tensor(0.6971, grad_fn=<MseLossBackward>)\n",
            "Epoch  22  loss:  tensor(0.6853, grad_fn=<MseLossBackward>)\n",
            "Epoch  23  loss:  tensor(0.6726, grad_fn=<MseLossBackward>)\n",
            "Epoch  24  loss:  tensor(0.6594, grad_fn=<MseLossBackward>)\n",
            "Epoch  25  loss:  tensor(0.6459, grad_fn=<MseLossBackward>)\n",
            "Epoch  26  loss:  tensor(0.6314, grad_fn=<MseLossBackward>)\n",
            "Epoch  27  loss:  tensor(0.6162, grad_fn=<MseLossBackward>)\n",
            "Epoch  28  loss:  tensor(0.6007, grad_fn=<MseLossBackward>)\n",
            "Epoch  29  loss:  tensor(0.5840, grad_fn=<MseLossBackward>)\n",
            "Epoch  30  loss:  tensor(0.5666, grad_fn=<MseLossBackward>)\n",
            "Epoch  31  loss:  tensor(0.5484, grad_fn=<MseLossBackward>)\n",
            "Epoch  32  loss:  tensor(0.5283, grad_fn=<MseLossBackward>)\n",
            "Epoch  33  loss:  tensor(0.5068, grad_fn=<MseLossBackward>)\n",
            "Epoch  34  loss:  tensor(0.4850, grad_fn=<MseLossBackward>)\n",
            "Epoch  35  loss:  tensor(0.4617, grad_fn=<MseLossBackward>)\n",
            "Epoch  36  loss:  tensor(0.4374, grad_fn=<MseLossBackward>)\n",
            "Epoch  37  loss:  tensor(0.4126, grad_fn=<MseLossBackward>)\n",
            "Epoch  38  loss:  tensor(0.3865, grad_fn=<MseLossBackward>)\n",
            "Epoch  39  loss:  tensor(0.3596, grad_fn=<MseLossBackward>)\n",
            "Epoch  40  loss:  tensor(0.3323, grad_fn=<MseLossBackward>)\n",
            "Epoch  41  loss:  tensor(0.3045, grad_fn=<MseLossBackward>)\n",
            "Epoch  42  loss:  tensor(0.2763, grad_fn=<MseLossBackward>)\n",
            "Epoch  43  loss:  tensor(0.2483, grad_fn=<MseLossBackward>)\n",
            "Epoch  44  loss:  tensor(0.2210, grad_fn=<MseLossBackward>)\n",
            "Epoch  45  loss:  tensor(0.1944, grad_fn=<MseLossBackward>)\n",
            "Epoch  46  loss:  tensor(0.1690, grad_fn=<MseLossBackward>)\n",
            "Epoch  47  loss:  tensor(0.1453, grad_fn=<MseLossBackward>)\n",
            "Epoch  48  loss:  tensor(0.1237, grad_fn=<MseLossBackward>)\n",
            "Epoch  49  loss:  tensor(0.1042, grad_fn=<MseLossBackward>)\n",
            "Epoch  50  loss:  tensor(0.0870, grad_fn=<MseLossBackward>)\n",
            "Epoch  51  loss:  tensor(0.0721, grad_fn=<MseLossBackward>)\n",
            "Epoch  52  loss:  tensor(0.0595, grad_fn=<MseLossBackward>)\n",
            "Epoch  53  loss:  tensor(0.0490, grad_fn=<MseLossBackward>)\n",
            "Epoch  54  loss:  tensor(0.0404, grad_fn=<MseLossBackward>)\n",
            "Epoch  55  loss:  tensor(0.0333, grad_fn=<MseLossBackward>)\n",
            "Epoch  56  loss:  tensor(0.0277, grad_fn=<MseLossBackward>)\n",
            "Epoch  57  loss:  tensor(0.0231, grad_fn=<MseLossBackward>)\n",
            "Epoch  58  loss:  tensor(0.0195, grad_fn=<MseLossBackward>)\n",
            "Epoch  59  loss:  tensor(0.0166, grad_fn=<MseLossBackward>)\n",
            "Epoch  60  loss:  tensor(0.0142, grad_fn=<MseLossBackward>)\n",
            "Epoch  61  loss:  tensor(0.0122, grad_fn=<MseLossBackward>)\n",
            "Epoch  62  loss:  tensor(0.0106, grad_fn=<MseLossBackward>)\n",
            "Epoch  63  loss:  tensor(0.0092, grad_fn=<MseLossBackward>)\n",
            "Epoch  64  loss:  tensor(0.0081, grad_fn=<MseLossBackward>)\n",
            "Epoch  65  loss:  tensor(0.0071, grad_fn=<MseLossBackward>)\n",
            "Epoch  66  loss:  tensor(0.0062, grad_fn=<MseLossBackward>)\n",
            "Epoch  67  loss:  tensor(0.0055, grad_fn=<MseLossBackward>)\n",
            "Epoch  68  loss:  tensor(0.0048, grad_fn=<MseLossBackward>)\n",
            "Epoch  69  loss:  tensor(0.0043, grad_fn=<MseLossBackward>)\n",
            "Epoch  70  loss:  tensor(0.0038, grad_fn=<MseLossBackward>)\n",
            "Epoch  71  loss:  tensor(0.0034, grad_fn=<MseLossBackward>)\n",
            "Epoch  72  loss:  tensor(0.0030, grad_fn=<MseLossBackward>)\n",
            "Epoch  73  loss:  tensor(0.0026, grad_fn=<MseLossBackward>)\n",
            "Epoch  74  loss:  tensor(0.0023, grad_fn=<MseLossBackward>)\n",
            "Epoch  75  loss:  tensor(0.0021, grad_fn=<MseLossBackward>)\n",
            "Epoch  76  loss:  tensor(0.0018, grad_fn=<MseLossBackward>)\n",
            "Epoch  77  loss:  tensor(0.0016, grad_fn=<MseLossBackward>)\n",
            "Epoch  78  loss:  tensor(0.0014, grad_fn=<MseLossBackward>)\n",
            "Epoch  79  loss:  tensor(0.0013, grad_fn=<MseLossBackward>)\n",
            "Epoch  80  loss:  tensor(0.0011, grad_fn=<MseLossBackward>)\n",
            "Epoch  81  loss:  tensor(0.0010, grad_fn=<MseLossBackward>)\n",
            "Epoch  82  loss:  tensor(0.0009, grad_fn=<MseLossBackward>)\n",
            "Epoch  83  loss:  tensor(0.0008, grad_fn=<MseLossBackward>)\n",
            "Epoch  84  loss:  tensor(0.0007, grad_fn=<MseLossBackward>)\n",
            "Epoch  85  loss:  tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "Epoch  86  loss:  tensor(0.0006, grad_fn=<MseLossBackward>)\n",
            "Epoch  87  loss:  tensor(0.0005, grad_fn=<MseLossBackward>)\n",
            "Epoch  88  loss:  tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "Epoch  89  loss:  tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "Epoch  90  loss:  tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "Epoch  91  loss:  tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "Epoch  92  loss:  tensor(0.0003, grad_fn=<MseLossBackward>)\n",
            "Epoch  93  loss:  tensor(0.0002, grad_fn=<MseLossBackward>)\n",
            "Epoch  94  loss:  tensor(0.0002, grad_fn=<MseLossBackward>)\n",
            "Epoch  95  loss:  tensor(0.0002, grad_fn=<MseLossBackward>)\n",
            "Epoch  96  loss:  tensor(0.0002, grad_fn=<MseLossBackward>)\n",
            "Epoch  97  loss:  tensor(0.0002, grad_fn=<MseLossBackward>)\n",
            "Epoch  98  loss:  tensor(0.0001, grad_fn=<MseLossBackward>)\n",
            "Epoch  99  loss:  tensor(0.0001, grad_fn=<MseLossBackward>)\n",
            "Epoch  100  loss:  tensor(0.0001, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAo6NRZ9Xht-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}